# ðŸ§  Endian RAG Assistant

This project is a lightweight **Retrieval-Augmented Generation (RAG)** prototype designed to support Sales Engineers and Key Account Managers at **Endian** by enabling natural language search over internal documents like sales decks, whitepapers, and proposals.

Built using:
- OpenAI GPT-4 / GPT-3.5 API
- Instructor-XL for embedding
- Chroma as local vector store
- Streamlit for an easy-to-use frontend

---

## ðŸš€ What It Does

- Reads and processes internal PDFs (`app/data`)
- Converts them into vector embeddings using `InstructorEmbedding`
- Stores them in a **local Chroma vector store**
- Allows you to **ask natural language questions** through a chat interface
- Answers are generated using OpenAI models, grounded in your documents

---

## ðŸ§± Project Structure

```text
RAG-ENDIAN/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ streamlit_app.py        # Streamlit UI
â”‚   â”œâ”€â”€ prompts/system_prompt.txt
â”‚   â””â”€â”€ data/                   # Input PDF documents
â”‚
â””â”€â”€ loaders/
    â””â”€â”€ pdf_loader.py           # PDF parsing logic
â”‚
â”œâ”€â”€ rag_engine/                 # Core logic for RAG pipeline
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ embedder.py             # Embedding using Instructor-XL
â”‚   â”œâ”€â”€ retriever.py            # Chroma vector search
â”‚   â””â”€â”€ rag_chain.py            # Prompt assembly + OpenAI call
â”‚
â”œâ”€â”€ vector_store/               # Persisted Chroma DB
â”‚   â””â”€â”€ chroma.sqlite3
â”‚
â”œâ”€â”€ run_pipeline.py             # CLI script to ingest new PDFs
â”œâ”€â”€ .env                        # OpenAI API key (not committed)
â”œâ”€â”€ requirements.txt            # Dependencies
â””â”€â”€ README.md                   # You're reading it!
```

## ðŸ§ª Setup Guide
1. Create a Virtual Environment
```
python -m venv rag-env
source rag-env/bin/activate  # or .\rag-env\Scripts\activate on Windows
```
2. Install Dependencies
```
pip install -r requirements.txt
```
3. Add Your OpenAI API Key
Create a .env file in the root directory:

```
OPENAI_API_KEY=sk-xxxxxxxxxxxxxxxxxxxx
```

## ðŸ“¥ Add and Ingest Documents
Place your PDFs inside app/data/

Run the following command to ingest them:

```
python run_pipeline.py
```

This will:

Load and chunk PDFs

Embed chunks using InstructorEmbedding

Store vectors in vector_store/ using Chroma

## ðŸ’¬ Start the Chatbot (Streamlit UI)
```
streamlit run app/streamlit_app.py
```

Then:

Go to http://localhost:8501

Ask questions like:

"What are the main features of the Secure Digital Platform?"

"Summarize our renewable energy use case."

See markdown-formatted answers sourced from the PDFs you uploaded

## ðŸ§  How It Works (Simplified)
```
graph TD
    A[PDF Files] --> B[Chunking & Embedding]
    B --> C[Chroma Vector Store]
    D[User Question] --> E[Embedding + Retrieval]
    C --> F[Relevant Chunks]
    F --> G[Prompt + Context]
    G --> H[OpenAI GPT-4/GPT-3.5]
    H --> I[Answer Displayed in UI]
```

## ðŸ›  Key Modules
File	Responsibility
pdf_loader.py	PDF parsing via PyMuPDF
embedder.py	Embedding with InstructorEmbedding
retriever.py	Vector search using Chroma
rag_chain.py	Prompt assembly + OpenAI completion
streamlit_app.py	Chat UI to interact with the system
run_pipeline.py	Command-line document ingestion

##Â âœ… Next Steps
Add support for DOCX, HTML or Confluence exports

Enable multi-user support + authentication

Integrate FAQs or Zendesk tickets for customer service

## ðŸ‘¤ Maintainer
Filippo Collini
Sales Engineer & Key Account Manager at Endian
GitHub

##Â ðŸ›¡ Security Note
This prototype uses local storage and only sends data to OpenAI's API for completions. Do not include sensitive or confidential client data unless secure APIs and environments are enforced.
